\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{mathtools,amssymb}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{layout}
\usepackage{geometry}
\usepackage{listings}
\usepackage{color}
\usepackage{pdfpages}
\newcommand*{\vertbar}[1][10ex]{\rule[-1ex]{0.5pt}{#1}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\newcommand{\norm}[2]{\left\lVert#1\right\rVert_#2}
\newcommand{\abs}[1]{\lvert#1\rvert}

\title{STATS 315A: Statistical Learning}
\author{Erich Trieschman}
\date{2022Q1 class notes}


\newcommand{\userMarginInMm}{8}
\geometry{
 left=\userMarginInMm mm,
 right=\userMarginInMm mm,
 top=\userMarginInMm mm,
 bottom=\userMarginInMm mm,
 footskip=5mm}

\newcommand*\circled[1]{\raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {#1}}}}
\newcommand*\bspace{$\; \bullet \;$}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
    
\lstset{frame=tb, language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3}

\begin{document}
\maketitle

% \tableofcontents

% TODO
\section{Supervised learning overview}
\subsection{Least squares}
\subsection{Nearest neighbors}
\subsection{Classes of restricted estimators}
\subsection{Bias-Variance tradeoff}
\subsection{Cross validation}
Cross validation is used to select the tuning parameters of a particular model, not the variables themselves. For example with subset selection, we use cross validaiton to select $s$, the subset size, \textbf{not} the actual predictors to use in the model\\
It would be ideal to have both a test set and a cross validation set. Running the CV and tuning parameters can bias the results. A separate test set provides convincing independent assessment
\subsubsection{K-fold cross validation}
\begin{itemize}
  \item For each k, fit the model with parameter $\lambda$ to the other K-1 parts, getting $\hat{\beta}^{-k}(\lambda)$
  \item Compute error, $RSS_{-k} = \sum_{i \in k}(y_i - x_i\hat{\beta}^{-k}(\lambda))^2$
  \item Cross validation error, $CV(\lambda) = \frac{1}{K}\sum_{k=1}^KRSS_{-k}(\lambda)$
\end{itemize}
\subsection{Bootstrap}
Sample N times with replacement from teh training set to form a bootstrap data set
Estimate model on bootstrap data, with predictions made from the original training data
Repeat process many times and average results
Poor estimate of prediction error (why?)
Good estimate for standard errors of predictions and confidence intervals for parameters



\section{Linear methods for regression}
Functions in the real world are rarely linear, but linear approximations are a good heuristic for the biance-variance tradeoff.

\subsection{Linear regression and least squares}
Assuming $X$ full rank. Geometrically, the point, $\hat{\beta}$ which solves $argmin_x\norm{X\hat{\beta} - y}{2}$ is one where $X\hat{\beta} - y$ is orthogonal to the range of $X$. To solve for this:
\begin{align*}
    \textrm{Want: } (X\hat{\beta} - y) &\perp \{z \vert z = X\hat{\beta}\} \longleftrightarrow (X\hat{\beta} - y) \perp range(A) \longleftrightarrow (X\hat{\beta} - y) \perp x_i, \forall i \in X\\
    x_i^T(X\hat{\beta} - y) &= 0, \; \forall i \in X \longleftrightarrow X^T(X\hat{\beta} - y) = 0 \longleftrightarrow \hat{\beta} = (X^TX)^{-1}X^TY
\end{align*}
\textbf{Properties:}
\begin{itemize}
  \item Regression coefficient $\hat{\beta}_i$ estimates the expected change in $y$ per unit change in $x_i$ \textit{holding all other predictors fixed}
  \item For $X_1, X_2$, mutually orthogonal matrices or vectors, the joint regression coefficients for $X = (X_1, X_2)$ on $y$, can be found from separate regressions. (Proof: $X_1^T(y - X\hat{\beta}) = X_1^T(y - X_1\hat{\beta}_1) = 0$)
  \item The multiple regression coefficient of $x_p$, the last column of $X$, is the same as the univariate coefficient in the regression of $y \sim z_p$. Here, $z_p = x_p - X_p^T\alpha$ (the part of $x_p$ orthogonal to $X_p$, all but column $x_p$ of $X$). Variance also comes form the univariate regression.
  \begin{itemize}
    \item $\hat{\beta_p} = (z_p^Tz_p)^{-1}z_p^Ty = z_p^Ty / z_p^Tz_p$
    \item $Var(\hat{\beta_p}) = \sigma^2 / z_p^Tz_p$
  \end{itemize}
\end{itemize}
\textbf{Assumptions:}
\begin{itemize}
  \item Errors, $\epsilon_i \sim N(0, \sigma^2)$ assumed to be \textit{independent} of the $x_i$'s
  \item $X$ considered fixed, not random. 
  \item $X$ is full rank. When not (because multiple variables are perfectly correlated), $X^TX$ is singular and the coefficients, $\hat{\beta}$, are not uniquely defined. In these cases, features can be reduced by filtering or with a regularization.
  \item Conditional expectation of $y$ is linear in $X$, $y = E(y \mid X) + \epsilon$. With this assumption, we can show $\hat{\beta} \sim N(\beta, (X^TX)^{-1}\sigma^2)$
\end{itemize}

\subsubsection{Standard error and confidence intervals}
We often assume $y_i = \hat{\beta}x_i + \epsilon_i$ with $E(\epsilon_i) = 0$ and $Var(\epsilon_i) = \sigma^2$. Then
\begin{align*}
  se(\hat{\beta}) = \biggl[ \frac{\sigma^2}{\sum(x_i - \bar{x})^2}\biggr]^{\frac{1}{2}} \textrm{, approximating with } \hat{se}(\hat{\beta}) = \biggl[ \frac{\hat{\sigma}^2}{\sum(x_i - \bar{x})^2}\biggr]^{\frac{1}{2}} \textrm{ where } \hat{\sigma}^2 = \frac{\sum(y_ - \hat{y_i})^2}{N-2}
\end{align*}

\subsubsection{Expectation of $\hat{\beta}$}
$Var(\hat{\beta}) = (X^TX)^{-1}\sigma^2$
$\hat{\sigma}^2 = \frac{1}{N - p - 1}\sum(y_i - \hat{y}_i)^2$. The $N-p-1$ denominator makes $\hat{\sigma}$ unbiased ($E(\hat{sigma}^2) = \sigma^2$)
% TODO -- add results from homework

\subsection{Subset selection}
Subset methods help us tradeoff an increase in bias with lower variance. Here we retain a subset of predictor variables for the final regression.
\textbf{Approaches:}
\begin{itemize}
  \item All subsets regression: finds the best subset of size $s \in \{1, \dots, p\}$ that minimizes the residual sum of squares. Limited use in high dimensions (>30) because of the computational complexity. 
  \item Forward stepwise selection: beginning with a model of the intercept only, sequentially add to the model the predictor that most reduces the residual sum of squares
  \item Backward stepwise selection: beginning with the full OLS model, sequentially remove from the model the predictor to most reduce residual sum of squares
\end{itemize}
\textbf{Note:} The tuning parameter, $s$ of each subset selection approach should be determined through cross validation

\subsection{Shrinkage methods}
\begin{itemize}
  \item Shrinkage methods often help us tradeoff an increase in bias with lower variance
  \item It is important to standardize (mean=0, variance=1) the predictors before running shrinkage methods to make the pentalty meaningful; centering also eliminates the need for an intercept
\end{itemize}
\subsubsection{Ridge regression}
Ridge regression is a linear regression with a square penalty on the size of the model parameters:
% TODO -- add the alternative form of beta hat from chapter 5
\begin{align*}
  \hat{\beta}^{ridge} &= argmin(y - X\beta)^T(y - X\beta) + \lambda\beta^T\beta\\
  \hat{\beta}^{ridge} &= (X^TX + \lambda I)^{-1}X^Ty
\end{align*}
This is a biased estimator for $y$ that may reduce MSE. Note when $\lambda = 0$, this is the same as OLS\\\\
% TODO - findings from HW1 related to the limit of lambda
Ridge regression shrinks the coefficients of the principal components ($Xv_j$), with relatively more shrinkage on the smaller components. Proof:
\begin{align*}
  X\hat{\beta} &= X(X^TX + \lambda I)^{-1}X^Ty\\
  X\hat{\beta} &= UDV^T(VD^2V^T + \lambda I)^{-1}VDU^Ty\\
  &= UD(D^2 + \lambda I)^{-1}DU^Ty\\
  &= \sum_{j=1}^pu_j \frac{d_j^2}{d_j^2 + \lambda}u_j^Ty
\end{align*}

\subsubsection{The Lasso}
The lasso is a shrinkage method for linear regressions like ridge, but uses the 1-norm as a penalty instead of the 2-norm. 
\begin{align*}
  \hat{\beta}^{lasso} = argmin(y - X\beta)^T(y - X\beta) + \lambda \norm{\beta}{1}
\end{align*}
There is no analytical solution to this objective, but the lasso is a convex problem when stated below (meaning it can be minimized)
\begin{align*}
  \textrm{min. } & argmin(y - X\beta)^T(y - X\beta)\\
  \textrm{subject to } & \lambda \norm{\beta}{1} \leq t
\end{align*}
We find with the lasso that the parameter vector often inclues zeros for specific parameters. Inuitively this makes sense since the pointed 1-norm ball is likely to be maximized at one of its corners.\\\\
\textbf{Elastic net} combines the ridge and lasso penalties through tuning parameter $\alpha$. It can be effective for sparse models with correlated predictors.
\begin{align*}
  \hat{\beta}^{enet} = argmin(y - X\beta)^T(y - X\beta) + (1-\alpha) \norm{\beta}{2} + \alpha \norm{\beta}{1}
\end{align*}

\subsection{Methods using derived input directions}
Here we choose a set of linear combinations of $x_i \in X$, and run a regression on these combinations
\subsubsection{Principal component regression}
Linear combinations are selected to maximize variance. These maximal-variance combinations are called the \textbf{principal components}. For standardized $X$, the principal components, $z_i$, are
\begin{align*}
  z_1 &= Xv \textrm{ such that $v$ maximizes } Var(Xv) = \frac{1}{N}v^TX^TXv \textrm{ subject to } \norm{v}{2} = 1\\
  z_i &= Xv_i \textrm{ where $v_i$ is the ith column of the SVD; singular values determine the ordering}
\end{align*}
The principal component analysis is highly connected to the singular value decomposition of standardized $X$. Since the SVD can help us construct the eigendecomposition of $X^TX$
\begin{align*}
  \frac{1}{N}X^TX = \frac{1}{N}VD^2V^T \Longleftrightarrow \frac{1}{N}V^TX^TXV = D^2
\end{align*}
Principal Component Analysis regression then generates a linear regression using a subset $s \leq p$ of the principal compoents. Since these principal components are orthogonal, the regression is a sum of univariate regressions

\subsubsection{Partial least squares}
Linear combinations are constructed using both $y$ and $X$, both standardized. 
\begin{itemize}
  \item Compute univariate regression coefficients, $\hat{\gamma}_l$ of $y$ on eacy $x_l$
  \item Construct $z_1 = \sum_l\hat{\gamma}_lx_l$
  \item Get $\hat{\beta}_1$ from $y \sim z_1$
  \item Orthogonalize $y, x_1, \dots, x_p$ with respect to $z_1$
  \begin{itemize}
    \item $y^* = y - \hat{\beta}_1z_1$
    \item $x^*_l = x_l - \frac{z_1^Tx_l}{z_1^Tz_1}z_1$
  \end{itemize}
  \item Repeat until $s \leq p$ directions have been obtained (we get back OLS if $s = p$)
\end{itemize}

\subsection{Degrees of freedom}
Degrees of freedom for linear regressions is the number of free parameters that determines the model. 
\begin{align*}
  \textrm{For } \hat{y} &= Hy \textrm{, } df = tr(H)\\
  \textrm{Note } \hat{y} &= X\hat{\beta} = X(X^TX)^{-1}X^Ty \textrm{ so } H = X(X^TX)^{-1}X^T \textrm{ in OLS}\\
  \hat{y} &= X\hat{\beta} = X(X^TX + \lambda I)^{-1}X^Ty \textrm{ so } H = X(X^TX + \lambda I)^{-1}X^T \textrm{ in Ridge regression}
\end{align*}
The lasso is not a linear regression. Its degrees of freedom are defined as
\begin{align*}
  df = \sum_i cov(y_i, \hat{y_i}) / \sigma^2
\end{align*}

% Todo2
\section{Linear methods for classification}
For classification, the input space can be divided into regions of constant classification, with decision boundaries
\begin{align*}
  \{ x \mid \hat{\beta}_kx = \hat{\beta}_lx \}
\end{align*}
In general, linear methods for classification model \textit{discriminant functions}, $\delta_k(x)$, or \textit{posterior probabilities}, $P(G=k \mid X=x)$, for each class, classifying $x$ to the class with the largest discriminant or probability.\\
For this theory, we require that these functions have some monotone transformation to a linear function; it turns out that both linear discriminant analysis and linear logistic regression result in linear log-odds (logits).
\subsection{Linear regression of indicator matrix}
Categorical $y \in \mathbb{R}^{n \times 1}$ is one-hot-encoded into a series of boolean vectors in $Y \in \mathbb{R}^{n\times k}$, and model parameters, $\hat{\beta} \in \mathbb{R}^{p \times k}$ are estimated in the same way
\begin{align*}
  \hat{\beta} &= (X^TX)^{-1}X^TY\\
  \hat{Y} &= X\hat{\beta} = X(X^TX)^{-1}X^TY\\
  \hat{G} &= argmax_{k\in \mathcal{G}} x^T\hat{\beta} \textrm{ for new observation } x
\end{align*}
\textbf{Notes}
\begin{itemize}
  \item Rigid nature of regression model, classes can be masked by other classes for $K > 2$
  \item A loose, but general rule is that if $K \geq 3$ classes are lined up in one direction, polynomial terms of degree $K - 1$ might be needed to resolve the classes. 
\end{itemize}


\subsection{Linear discriminant analysis (LDA)}
Category of backward selection models, meaning we use $y$ to generate boundaries of $X$. Suppose $f(x \mid k)$ is the density of $X$ conditional on class $G=k$, and $\pi_k$ is the prior probability for class $G=k$. Bayes rule says
\begin{align*}
  P(G=k \mid X=x) = \frac{f(x\mid k) \pi_k}{\sum_{l=1}^Kf(x \mid l)\pi_l}
\end{align*}
\textbf{Linear discriminant analysis} and its relatives are based on the assumption that $f(x \mid k)$ is multivariate Gaussian
\begin{align*}
  f(x \mid k) = \frac{1}{(2\pi)^{p/2}\abs{\Sigma_k}^{1/2}}\exp(-\frac{1}{2}(x - \mu_k)^T \Sigma_k^{-1}(x - \mu_k))
\end{align*}
\subsubsection{Linear discriminant analysis}
LDA specifically arises out of the special case when we assume all classes have a common variance, i.e., $\Sigma_k = \Sigma \forall k$. This results in sufficient cancelations when comparing classes, leading to an equation linear in $x$
\begin{align*}
  \log\frac{P(G=k \mid X=x)}{P(G=k \mid X=x)} &= \log\frac{f(x \mid k)}{f(x \mid l)} + \log\frac{\pi_k}{\pi_l}\\
  &= \log\frac{\pi_k}{\pi_l} - \frac{1}{2}(\mu_k + \mu_l)^T\Sigma^{-1}(\mu_k - \mu_l) + x^T\Sigma^{-1}(\mu_k - \mu_l)
\end{align*}
\textbf{Notes}
\begin{itemize}
  \item This implies the decision boundary between classes is linear in $x$
  \item These decision boundaries are \textit{not} perpendicular bisectors to the line segments joining centroids (this would be the case if $\Sigma = \sigma^2\mathbf{I})$ and the priors, $\pi_i$ were equal)
  \item In practice we don't know the parameters of the Gaussian distributions and we estimate with
  \begin{itemize}
    \item $\hat{\pi}_k = n_k / N$
    \item $\hat{\mu}_k = \sum_{g_i = k}x_i/n_k$
    \item $\hat{\Sigma}_k = \sum_{k=1}^K\sum_{g_i=k}(x_i - \hat{\mu}_k)^T(x_i - \hat{\mu}_k)/(N - K)$
  \end{itemize}
  \item In the case of a binary $y$, LDA and linear regression have a direct correspondence
\end{itemize}


\subsubsection{Quadratic discriminant analysis (QDA)}
If we tighten our assumption about $\Sigma_k$ so that the covariance matrices are not even across groups, we get a \textbf{Quadratic discriminant analysis} that is quadratic in $x$:
\begin{align*}
  \delta_k(x) = \frac{1}{2}\log\abs{\Sigma_k} - \frac{1}{2}(x - \mu_k)^T\Sigma_k^{-1}(x - \mu_k) + log\pi_k
\end{align*}
\textbf{Notes}
\begin{itemize}
  \item Decision boundaries between classes are described by a quadratic equation: $\{x \mid \delta_k(x) = \delta_l(x) \}$
  \item We can produce quadratic boundaries with LDA as well by enlarging the parameter space to include quadratics ($(x_1, x_2) \Longrightarrow (x_1, x_2, x_1x_2, x_1^2, x_2^2))$. In general, QDA is the preferred approach
\end{itemize}

\subsubsection{Regularized discriminant analysis (RDA)}
Suposing we want to compromise between LDA and QDA, \textbf{regularized discriminant analysis} allows us to tune in between our assumptions of common across-class covariance (LDA) and unique within-class covariance (QDA) with a tuning parameter, $\alpha$. Regularized covariance matrices have the form
\begin{align*}
  \hat{\Sigma}_k(\alpha) = \alpha\hat{\Sigma}_k + (1 - \alpha)\hat{\Sigma}
\end{align*}
We can also consider a regularization approach to tune a caommon across-class covariance to an assumption about independence with tuning parameter, $\gamma$
\begin{align*}
  \hat{\Sigma}(\gamma) = \gamma\hat{\Sigma} + (1 - \gamma)\hat{\sigma}^2\mathbf{I} 
\end{align*}

\subsubsection{Additional LDA notes}
We can reduce the computational complexity of LDA by taking advantage of the eigendecomposition of the covariance matrix: $\hat{\Sigma} = UDU^T$, noticing
\begin{align*}
  (x - \hat{\mu}_k)^T \Sigma^{-1}(x - \hat{\mu}_k) &= [U^T(x - \hat{\mu}_k)]^T D^{-1}[U^T(x - \hat{\mu}_k)]\\
  \log\abs{\hat{\Sigma}} &= \sum_l\log d_{ll}
\end{align*}
With these relations, $X$ can be classified through two steps
\begin{itemize}
  \item Sphere data: $X^* = D^{-1/2}U^TX$ where $\hat{\Sigma} = UDU^T$ (note now that the common covariance matrix is $\mathbf{I}$)
  \item Classify the closest class centroid in the transformed space, modulo the effect of class probabilities, $\pi_k$
\end{itemize}

\subsection{Logistic Regression}



% Todo3
\section{Basis expansions and regularizations}
\subsection{Piecewise polynomials and regression splines}
\subsection{Smoothing splines}
\subsection{Multidimentional splines}
\subsection{Regularization and reproducing kernel Hilbert spaces}


\end{document}
