\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{mathtools,amssymb}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{layout}
\usepackage{geometry}
\usepackage{listings}
\usepackage{color}
\usepackage{pdfpages}
\newcommand*{\vertbar}[1][10ex]{\rule[-1ex]{0.5pt}{#1}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\newcommand{\norm}[2]{\left\lVert#1\right\rVert_#2}
\newcommand{\abs}[1]{\lvert#1\rvert}

\title{CME308: Stochastic Methods in Engineering}
\author{Erich Trieschman}
\date{2022 Spring quarter}

\newcommand{\userMarginInMm}{8}
\geometry{
 left=\userMarginInMm mm,
 right=\userMarginInMm mm,
 top=\userMarginInMm mm,
 bottom=\userMarginInMm mm,
 footskip=5mm}

\newcommand*\circled[1]{\raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {#1}}}}
\newcommand*\bspace{$\; \bullet \;$}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
    
\lstset{frame=tb, language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3}


\begin{document}
\maketitle

\section{Probability}
\subsection{Calculus cheat sheet}
\textbf{Logs:} $log_b(M * N) = log_bM + log_bN$ \bspace $log_b(\frac{M}{N}) = log_bM - log_bN$\bspace $log_b(M^k) = klog_bM$\bspace $e^ne^m = e^{n+m}$\\
\textbf{Derivatives:} $(x^n)' = nx^{n-1}$\bspace $(e^x)' = e^x$ \bspace $(e^{u(x)})' = u'(x)e^x$ \bspace $(log_e(x))' = (lnx)' = \frac{1}{x}$\bspace $(f(g(x)))' = f'(g(x))g'(x)$\\
\textbf{Integrals: } $\int_a^b f(x)dx = \int_{g(a)}^{g(b)}f(g(u))g'(u)du$ where $g(u) = x$\bspace $\int_a^b u(x)v'(x)dx = u(b)v(b) - u(a)v(a) - \int_a^b u'(x)v(x)dx$\\
\textbf{Infinite series and sums:} $e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \dots = \sum_{n=0}^\infty \frac{x^n}{n!}$\bspace $(1 + \frac{a}{n})^n \longrightarrow e^a$\\
$ln(1 + x) = 1 - x + \frac{x^2}{2} - \frac{x^3}{3} + \dots = \sum_{n=0}^\infty (-1)^n\frac{x^n}{n}$\bspace $\frac{1}{1-x} = 1 + x + x^2 + x^3 + \dots = \sum_{n=0}^\infty a^x$ for $\abs{x} < 1$

\subsection{Expectation}
\textbf{Conditional expectation: } $p_{X|Y}(x|y) = \frac{p_{x,y}(x,y)}{p_y(y)}$\\
\textbf{Bayes theorem: } $P(E_i \mid B) = \frac{P(B \mid E_i)P(E_i)}{\sum_{j=1}^\infty P(B \mid E_j)P(E_j)} = \frac{P(B \mid E_i)P(E_i)}{P(B)}$, where $E_1, E_2, \dots$ form a partition of the sample space.\\
\textbf{Expectation: } $E(X) = \sum_x xP(X=x)$, also written $E(X) = \sum_{x \in S} X(s)p(s)$, where $p(s)$ is the probability that element $s \in S$
\begin{itemize}
    \item $E(g(X)) = \sum_i g(x_i)p_X(x_i)$ \bspace $E(aX + b) = aE(X) + b$ \bspace $E(X + Y) = E(X) + E(Y)$
\end{itemize}
\textbf{Variance: } $Var(X) = E((X - E(X)))^2) = \sigma^2$
\begin{itemize}
    \item $Var(X) = E(X^2) - \mu^2$ \bspace $Var(aX+b) = a^2Var(X)$ \bspace $Var(X + Y) = Var(X) + Var(Y)$ for $X,Y$ independent \bspace 
\end{itemize}
\textbf{Covariance: } $Cov(X, Y) = E((X - E(X)(Y - E(Y)) = E(XY) - E(X)E(Y)$
\begin{itemize}
    \item $Cov(X, X) = Var(X)$ \bspace $Cov(aX, bY) = abCov(X, Y)$ \bspace $Cov(X, Y+Z) = Cov(X, Y) + Cov(X, Z)$ \bspace $Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)$
\end{itemize}
\textbf{Law of iterated expectation: } $E(E(Y\mid X)) = E(Y)$\\
\textbf{Proof: }$E(Y\mid X) = \sum_y y\frac{f_{X, Y}(X, y)}{f_X(X)}, \; \; E(E(Y\mid X)) = \sum_x \sum_y \left ( y\frac{f_{X, Y}(x, y)}{f_X(x)} \right ) f_X(x) = \sum_x \sum_y y f_{X, Y}(x, y) = \sum_y y f_{Y}(y) = E(Y)$\\
\textbf{Law of total probability}: $P(E) = \sum_{i=-\infty}^\infty P(E \mid X = x)P(X) \textrm{ and } P(E) = \int_{-\infty}^\infty P(E \mid X=x)f(x)dx$\\
\textbf{Variance decomposition formula: } $Var(Y) = E(Var(Y\mid X)) + Var(E(Y \mid X))$\\
\textbf{Cauchy-Schwartz inequality: }$E(UV)^2 \leq E(U^2)E(V^2) \textrm{, with equality if } P(cU=U) = 1 \textrm{ for some constant, } c$\\
\textbf{Transformations of random variables: } For $X$ with density $f_X$ and $Y = g(X)$\\
$F_Y(y) = P(g(X)\leq y) = P(X \leq g^{-1}(y)) = F_X(g^{-1}(y))$ \bspace $f_Y(y) = \frac{d}{dy}F_X(g^{-1}(y)) = f_X(g^{-1}(y))\abs{\frac{d}{dy}g^{-1}(y)}$
 
\subsection{Inequalities}
\textbf{Jensen inequality: } $E(g(x)) \geq g(E(x))$ for $g(x)$ convex\\
\textbf{Markov inequality: } For $X\geq 0 \;, \;P(X \geq t) \leq \frac{E(X)}{t} \; \; \forall t>0$. \textbf{Proof: }\\
\begin{align*}
    \textrm{Let } Y &= \begin{cases}
        1 & X \geq t\\
        0 & \textrm{otherwise}
    \end{cases}, \textrm{  Then } tY \leq X \textrm{ since } \begin{cases}
        X \geq t & t*1 \leq X \\
        X < t & t*0 < X
    \end{cases}\\
    tY &\leq X \Longrightarrow E(tY) \leq E(X) \Longrightarrow tP(X \geq t) \leq E(X)) \Longrightarrow P(X \geq t) \leq \frac{E(X)}{t}
\end{align*}
\textbf{Chebyshev inequality: } $P(\abs{X - E(X)} \geq t) \leq \frac{Var(X)}{t^2} \; \; \forall t > 0$. \textbf{Proof: }\\
\begin{align*}
    P(\abs{X - E(X)} \geq t) = P((X - E(X))^2 \geq t^2) &\leq \frac{E((X - E(X))^2)}{t^2} = \frac{Var(X)}{t^2} \textrm{, by Markov inequality}
\end{align*}
\textbf{Exponential inequality: } $P(X > a) \leq e^{-\theta a} E(e^{\theta X})$ for all $\theta > 0$. \textbf{Proof:}\\
\begin{align*}
    P(X > a) &= P(\theta X > \theta a) \textrm{ for } \theta > 0 \Longrightarrow P(e^{\theta X} > e^{\theta a}) \leq e^{-\theta a} E(e^{\theta X}) \textrm{, by Markov inequality}
\end{align*}
\textbf{(Corrolary) Upper bound on large deviations: } $P(S_n < na) \leq e^{-nI(x)}$. \textbf{Proof: }\\
\begin{align*}
    P(S_n > a) &\leq e^{-\theta a} E(e^{\theta S_n}) \textrm{, by exponential inequality for } S_n = \sum_i X_i (iid)\\
    &= e^{-\theta a} \prod_i E(e^{\theta X_i}) = e^{-\theta a} E(e^{\theta X_1})^n \textrm{, by iid}\\
    &= e^{-\theta a + n \psi(\theta)} \textrm{, where } \psi(\theta) = \log{Ee^{\theta X_1}} \textrm{, the log of the MGF}\\
    P(S_n > na) &= e^{-n(\theta(x) a - n \psi(\theta(x))} \textrm{, minimizing RHS w.r.t } \theta\\
    &= e^{-nI(x)} \textrm{ where } I(x) = \theta(x) a - n \psi(\theta(x))
\end{align*}

\subsection{Stationarity}
In some cases, $X_i$ may not be $i.i.d$, but there may still exist a statistical equilibrium:
\begin{itemize}
    \item $\{X_1, \dots, X_n\} \overset{d}{=} \{X_{m+1}, \dots, X_{m+n}\}$ \bspace $EX_1 = EX_n$ \bspace $Cov(X_1, X_n) = Cov(X_{m+1}, X_{m+n})$, called $c(n)$ where $n$ is lag
\end{itemize}
We can prove $c(n) \overset{p}{\longrightarrow} 0$ using Chebychev and solving for $Var \overline{X}_n$ as a function of $c(n)$, leading to the bound (which goes to 0): 
\begin{align*}
    P(\abs{\overline{X}_n - EX_1} > \epsilon) \leq \frac{2}{n}\sum_i c(i)
\end{align*}



\subsection{Generative functions}
\subsubsection{Characteristic functions}
\begin{itemize}
    \item The \textbf{characteristic function} of X is $\phi_X(t) = E \exp(itX)$
    \item \textbf{Common characteristic functions:} $N(\mu, \sigma^2): \exp(it\mu - \frac{1}{2}\sigma^2t^2)$ \bspace $Exp(\lambda): (1 - it\lambda ^{-1})^{-1}$ \bspace $Poisson(\lambda): \exp{\lambda(e^{it} - 1)}$
    \item \textbf{Properties:} $E[X^k] = i^{-k}E[X^k]$ \bspace $\phi_{a_1X_1 + \dots + a_nX_n}(t) = \phi_{X_1}(a_1t) \dots \phi_{X_n}(A_nt)$ for $X_i$ indep.
    \item The \textbf{moment-generating function} of X is $M_X(t) = E\exp(tX)$
\end{itemize}



\subsection{Weak law of large numbers}
For $X_1, X_2, \dots, X_n$ i.i.d. with $E(X_i) = \mu$,  $Var(X_i) = \sigma^2$, $\overline{X}_n = \frac{1}{n}\sum_{i = 1}^n X_i$, then for any $\epsilon > 0$
\begin{align*}
	P(\abs{\overline{X}_n - \mu} > \epsilon) \leq \frac{Var(\overline{X}_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \rightarrow 0 \textrm{ as } n \rightarrow \infty \textrm{, by Chebyshev inequality}, \; \; \; X_1 + \dots + X_n = S_n\approx E S_n \textrm{, the "meta result"}
\end{align*}


\subsection{Central limit theorem}
\begin{align*}
	&\sqrt{n}\frac{(\overline{X}_n - \mu)}{\sigma} \longrightarrow N(0, 1) \Longleftrightarrow \sqrt{n}(\overline{X}_n - \mu) \longrightarrow N(0, \sigma^2) = \sigma N(0, 1), \; \; X_i + \dots + X_n = S_n \approx N(E S_n, Var S_n) \textrm{, the "meta result"}\\
    &\lim_{n \rightarrow \infty}P(\frac{S_n}{\sigma \sqrt{n}} \leq x) = \Phi(x) \textrm{, for } S_n = \sum_{i=1}^nX_i, \; \; X_1, X_2, \dots, X_n \textrm{i.i.d. with } E(X_i) = 0 \textrm{ (WLOG), } Var(X_i) = \sigma^2
\end{align*}
\textbf{Proof sketch:} Start with $M_{S_n}(t)$, plug in $t / (\sigma\sqrt(n))$, and use Taylor expansion to show convergence to the MGF of a normal random variable, $e^{\frac{t^2}{2}}$
\textbf{Monte Carlo: }\bspace Sample $Y \in \mathbb{R}^d$ \bspace Compute $X = g(Y)$ \bspace Repeat $n$ times \bspace form $\overline{X}_n$ and use CLT for asymptotic behavior

\subsubsection{Delta method}
If $g$ is a differentiable function at $\mu$, $\sqrt{n}(g(\bar{X}_n) - g(\mu)) \overset{d}{\longrightarrow} N(0, g'(\mu)^2\sigma^2) = g'(\mu)\sigma N(0,1)$\\
\textbf{Proof sketch:} Start with Taylor expansion $g(\bar{X}_n) \approx g(\mu) + g'(\mu)(\bar{X}_n - \mu)$ and rearrange to get $ \sqrt{n}(g(\bar{X}_n) - g(\mu)) \approx g'(\mu)\sqrt{n}(\bar{X}_n - \mu) \overset{d}{\longrightarrow} N(0, g'(\mu)^2\sigma(2))$. 
\textbf{Note:} if we find that $g'(\mu) = 0$, then repeat this process with the second derivative, $g''(\mu)$.


\subsubsection{Convergence in probability}
\textbf{Convergence in probability: } $X_n \overset{p}{\longrightarrow} X$ when $P(\abs{X_n - X} > \epsilon) \longrightarrow 0 \textrm{ as } n \longrightarrow \infty$\\
\textbf{Continuous mapping theorem: } if $X_n \overset{p}{\longrightarrow} X$ and $g$ a continuous function then $g(X_n) \overset{p}{\longrightarrow} g(X)$\\
\textbf{Consistent estimator:} $T_n = T_n(X_1, \dots, X_n)$ converges in probability to $g(\theta)$, a function of the model parameter\\
\textbf{Bounded convergence theorem:} $Z_n \overset{p}{\longrightarrow} Z_\infty, \abs{Z_n} \leq c < \infty \Longrightarrow EZ_n \overset{p}{\longrightarrow} EZ_\infty$\\
Proof starts with $\abs{E(Z_n - Z_\infty)}$ and uses i) triangle inequality, ii) indicator functions for the case when difference is $>\epsilon, <\epsilon$\\
\textbf{Dominated convergence theorem:} $Z_n \overset{p}{\longrightarrow} Z_\infty, E\beta < \infty, \abs{Z_n(\omega)} \leq \beta(\omega) \forall \omega \Longrightarrow EZ_n \overset{p}{\longrightarrow} EZ_\infty$\\
\textbf{Fatous Lemma: } for $Z_n > 0$, $E\lim_{n \rightarrow \infty} Z_n \leq \lim_{n \rightarrow \infty} E Z_n$

\subsubsection{Convergence in distribution (a.k.a. weak convergence)}
\textbf{Convergence in distribution: } $X_n \overset{d}{\longrightarrow} X$ when $\lim_{n\longrightarrow \infty} F_{X_n}(x) = F_X(x)$ at all continuity points in $F_X$\\
\textbf{Equalities: } $X_n \overset{d}{\longrightarrow} X \Longleftrightarrow Eh(Z_n) \overset{p}{\longrightarrow} Eh(Z_\infty) \forall h$, bounded/continuous $\Longleftrightarrow \phi_{Z_n}(t) \overset{p}{\longrightarrow} \phi_{Z_\infty}(t) \forall t$\\
\textbf{Confidence intervals: } $P(Z_{\alpha \div 2} \leq Z \leq Z_{1-\alpha \div 2}) = P(\frac{\hat{\sigma}}{\sqrt{n}}Z_{\alpha \div 2} \leq \bar{X}_n - \mu \leq \frac{\hat{\sigma}}{\sqrt{n}}Z_{1-\alpha \div 2}) = P(\mu \in \left[\bar{X}_n \pm \frac{\hat{\sigma}}{\sqrt{n}}Z_{1-\alpha \div 2}\right]) = 1-\alpha$\\
\textbf{Slutsky's lemma}
$A_nX_n + B_n \overset{d}{\rightarrow} aX + b$ if $\{X_n\}$ sequence, $X_n \overset{d}{\rightarrow} X$,  $\{A_n\}$ sequence, $A_n \overset{d}{\rightarrow} A$, $\{B_n\}$ sequence, $B_n \overset{b}{\rightarrow} b$

\subsubsection{Almost sure convergence}
$P(\omega : \lim_{n\rightarrow \infty}X_n(\omega) \overset{p}{\rightarrow} X_\infty(\omega)) = 1$ where $\omega$ is element in set of all sequences \bspace P($\limsup_{n \rightarrow \infty}\{\abs{X_n - X_\infty}\} > \epsilon) = 0$

\subsection{Theory of large deviations}
\textbf{Variance reduction: } $EX = \int_{-\infty}^\infty x f(x)dx = \int_{-\infty}^\infty x f(x) h(x) / h(x) = E[Zf(x)/h(x)]$ where $h(z)$ is pdf of $Z$. $f(x) / h(x)$ known as the likelihood ratio.\\
\textbf{Importance sampling:} Choose $h(x)$ to minimize variance. Minimal $H(dx)$ turns out to be the conditional probability of the event happening on event happening: $H^*(dx) = \mathbb{I}\{A\}(x)F(dx)/F(A)$


\subsubsection{Ergotic theorem }
\subsubsection{Cramer-Rao Bound}
\subsection{Censoring data}
\subsection{Estimating equations}



\section{Statistics}
\subsection{Method of moments estimator}
\begin{itemize}
    \item $E(X^k) = g(\theta)$ \bspace Calculate moment with MGF, lower moments typically lead to estimators with lower asymptotic variance
    \item $g^{-1}(E(X^k)) = \theta$ \bspace Invert this expression to create an expression for the parameter(s) in terms of the moment
    \item $\hat{\theta} = g^{-1}(\frac{1}{n}\sum X_i^k)$ Insert the sample moment into this expression, thus obtaining estimates of the parameters in terms of data
    \item $\sqrt{n}(g^{-1}(\frac{1}{n}\sum X_i^k) - \theta) \overset{d}{\longrightarrow}  N(0, f'(E(X_i^k))^2Var(X_i^k)^2)$ Use the delta method
    \item If multiple parameters characterize the distribution, use multiple moments and a system of equations
\end{itemize}

\subsection{Maximum likelihood estimator}
\begin{itemize}
    \item $L(\theta) = \prod_{i=1}^nf(X_i, \theta)$ \bspace Construct the likelihood function
    \item $log(L(\theta)) = l(\theta) = \sum_{i=1}^nlog(f(X_i, \theta))$ \bspace Take the log of the likelihood
    \item Find critical points of this function 
    \item Find critical points of this function (e.g., $0 = \sum_{i=1}^n\frac{d}{d\theta}log(f(X_i, \hat{\theta}))$) and determine that one is a maximum
\end{itemize}
\textbf{Approach to constructing MLE when indicators, $\mathbb{I}\{U\}$, are present:} Logs of indicators and derivatives of indicators are very difficult to work with \bspace Simplify likelihood function (splitting indicators when possible) \bspace Make an argument for why the function is increasing or decreasing \bspace Determine the value at the bounds of the function


\section{Examples}
\subsection{Newsvendor problem}



\end{document}
